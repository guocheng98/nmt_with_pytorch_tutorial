{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5690464-5ce2-47f6-9000-6054b5138391",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data And torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a34b0-2d96-4ff5-b8f3-be10c2c6873a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Intro to DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7959ad-2827-43ed-a125-b850d6e6cef7",
   "metadata": {},
   "source": [
    "Data manipulation and model/training may seem separable; however, it is often not the case. Before we dive into model construction or training, we need to first make decisions on how our (text) data -- possibly saved in one or multiple files -- should be read and processed.\\\n",
    "\\\n",
    "Let's first look at this <code>train_epoch</code> function from PyTorch's documentation. (Extracted 2023/01/06)\\\n",
    "\\\n",
    "We can see here that a <code>torch.utils.data.DataLoader</code> object is created with some kind of a data iterator, batch size, and a collate function.\\\n",
    "According to PyTorch's <a href='https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader'>documentation</a>, <code>DataLoader</code> class is at the heart of PyTorch data loading utility. It is useful for batching, customizing data sampling, parallelizing data loading, and fast data transfer with memory pinning.\\\n",
    "\\\n",
    "It seems complicated already, but essentially, a <code>DataLoader</code> is just an iterator that gives you a processed <strong>batch</strong> each time from your actual data. With that in mind, we can continue to see what we need to construct a <code>DataLoader</code> for machine translation task.\n",
    "<p style=\"text-align:center;\"><img src='./images/train_epoch_codesnippet.png' style='max-width: 50%; height: auto;'></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b42bb-a298-4b68-a8ec-7d6c1b9261d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc6ccb-2bdc-41f4-b489-c5c6e1b3b95a",
   "metadata": {},
   "source": [
    "A <code>dataset</code> is the most important argument of <code>DataLoader</code> constructor. The <code>dataset</code> object is not your actual data, rather, it should be seen as a Python object created with the actual data or the path to your data.\\\n",
    "\\\n",
    "<code>torch.utils.data.DataLoader</code> supports two types of datasets. Either way, we need to write our own <code>dataset</code> class.\n",
    "<ul>\n",
    "    <li>map-style dataset: an instance of a subclass of <code>torch.utils.data.Dataset</code></li>\n",
    "    <li>iterable-style dataset: an instance of a subclass of <code>torch.utils.data.IterableDataset</code></li>\n",
    "</ul>\n",
    "More detail see PyTorch's <a href='https://pytorch.org/docs/stable/data.html#dataset-types'>documentation.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97870ff3-6ff4-4ca3-9c99-2bd8f9dea444",
   "metadata": {},
   "source": [
    "For this notebook, we have file based data saved on local disk. Since I do not want to load all the data into the memory at once, I choose to create a iterable-styple dataset.\\\n",
    "\\\n",
    "When subclassing <code>torch.utils.data.IterableDataset</code>, it is required to implement custom <code>\\_\\_iter\\_\\_</code> function. I highly recommend reading this <a href='https://medium.com/swlh/how-to-use-pytorch-dataloaders-to-work-with-enormously-large-text-files-bbd672e955a0'>article</a> on subclassing <code>IterableDataset</code>.\\\n",
    "\\\n",
    "Our data consists of tab-separated source target bitext per line, so we all do some processing when reading each line, hence the <code>def _line_split</code> and the leverage of <code>map()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22824d34-74c8-4e3a-ad97-f027a434204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class MyIterableDataset(IterableDataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        \n",
    "    def _line_split(self, line):\n",
    "        src, tgt = line.strip('\\n').split('\\t')\n",
    "        return src, tgt\n",
    "    \n",
    "    def __iter__(self):\n",
    "        file_iter = open(self.file_path, 'r', encoding='utf-8')\n",
    "        mapped_iter = map(self._line_split, file_iter)\n",
    "        return mapped_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307a0874-8f8d-442f-970d-8c6a013901c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = MyIterableDataset('./data/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9896ef1a-e2f5-4257-a279-252223cfbbbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### little test on MyIterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8536e6ce-e647-48ba-918d-ea4c161a64d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this is a test', '这是个测试')\n"
     ]
    }
   ],
   "source": [
    "for pair in train_iter:\n",
    "    print(pair)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3f034-fb8b-4af7-8bb7-a36d6f5be78f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Collate function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076f1a3-31dc-4120-ae78-3f537d6085ff",
   "metadata": {},
   "source": [
    "The second building block of a <code>DataLoader</code> is <code>collate_fn</code>. Basically, <code>collate_fn</code> is a callable that takes in a raw batch (with typing as <code>List</code>) of return results from <code>dataset</code> as argument and transform it into desired output format.\\\n",
    "\\\n",
    "You can do some processing on the raw batch and also define how you want to create your batch. If not defined, PyTorch only put <code>batch_size</code> examples together using <code>torch.stack</code>.\\\n",
    "\\\n",
    "In our case, <code>dataset</code> is only returning a tuple of raw string <code>(src, tgt)</code>, which we cannot directly pass into a <em>transformer</em> model. We still need:\n",
    "<ul>\n",
    "    <li>tokenization</li>\n",
    "    <li>numericalization</li>\n",
    "    <li>adding BOS and EOS token</li>\n",
    "    <li>padding: we cannot make batch if tensors' length is not the same</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4126f-eca3-4ef0-ac81-7f23bfe20c90",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### tokenization and numericalization (torchtext.vocab.Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991126c2-23b7-4524-b86c-fef0319653b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I already trained the tokenizers with SentencePiece\n",
    "import sentencepiece as spm\n",
    "\n",
    "src_tokenizer = spm.SentencePieceProcessor(model_file='./tokenizer/tokenizer_sp_en')\n",
    "tgt_tokenizer = spm.SentencePieceProcessor(model_file='./tokenizer/tokenizer_sp_zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca4136-fe44-4986-9425-00b8d1374d4c",
   "metadata": {},
   "source": [
    "Numericalization depends on creating a <code>torchtext.vocab.Vocab</code> object. See <a href='https://pytorch.org/text/0.14.0/vocab.html'>Documentation</a>.\\\n",
    "\\\n",
    "Two major methods to build the <code>Vocab</code>:\n",
    "<ul>\n",
    "    <li>factory method from a <code>Ordered Dictionary</code> which maps tokens to occurance frequencies</li>\n",
    "    <li>build_vocab_from_iterator</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60c9c7-fb4b-4f2b-8d46-0e7fce536e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" factory method, only as an example \"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(text, index, tokenizer):\n",
    "  counter = Counter()\n",
    "  for s in text:\n",
    "    counter.update(tokenizer.encode(s, out_type=str))\n",
    "  return vocab(counter, min_freq=10, specials=special_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8ab35-91e7-4845-9570-69393b5676ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we already created the train_iter, we will use this method\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "def yield_tokens(bitext_iter, is_src: bool):\n",
    "    for bitext in bitext_iter:\n",
    "        if is_src:\n",
    "            yield src_tokenizer.encode(bitext[0], out_type=str)\n",
    "        else:\n",
    "            yield tgt_tokenizer.encode(bitext[1], out_type=str)\n",
    "\n",
    "src_vocab = build_vocab_from_iterator(yield_tokens(train_iter, is_src=True),\n",
    "                                     min_freq=10,\n",
    "                                     specials=special_symbols,\n",
    "                                     special_first=True)\n",
    "tgt_vocab = build_vocab_from_iterator(yield_tokens(train_iter, is_src=False),\n",
    "                                     min_freq=10,\n",
    "                                     specials=special_symbols,\n",
    "                                     special_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5112246-86df-49b3-8a45-8f62904a11d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### little test on Vocab objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "796e71d3-151b-4949-ae91-14e90b897aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁or'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab.lookup_token(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d520fb15-cbd1-4075-ae31-1fc9d561ff61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab.lookup_token(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06623fc5-48cd-411e-bd86-dbae77b67368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'奢华'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_vocab.lookup_token(394)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1bd039f-d1dd-4325-9ee3-ef794b2e333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁transform', 'er', '▁is', '▁a', '▁Se', 'q', '2', 'Se', 'q', '▁model', '▁introduced', '▁in', '▁“', 'att', 'ention', '▁is', '▁all', '▁you', '▁need', '”', '▁paper', '▁for', '▁sol', 'ving', '▁machine', '▁translation', '▁tasks', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4099,\n",
       " 509,\n",
       " 18,\n",
       " 11,\n",
       " 3828,\n",
       " 5650,\n",
       " 19,\n",
       " 14670,\n",
       " 5650,\n",
       " 145,\n",
       " 1479,\n",
       " 12,\n",
       " 71,\n",
       " 5275,\n",
       " 17046,\n",
       " 18,\n",
       " 61,\n",
       " 34,\n",
       " 456,\n",
       " 108,\n",
       " 1559,\n",
       " 20,\n",
       " 7974,\n",
       " 7404,\n",
       " 2409,\n",
       " 16672,\n",
       " 4287,\n",
       " 6]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = src_tokenizer.encode('transformer is a Seq2Seq model introduced in “attention is all you need” paper for solving machine translation tasks.', out_type=str)\n",
    "print(tokenized)\n",
    "src_vocab(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b142591-ef9e-49f4-aa83-6a1203165327",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### set default index for OOV token\n",
    "We also need to assign default token index for OOV token, which should be the index of <unk> in this case (and many other cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24762b72-bc59-44ad-a327-be805f2da558",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab.set_default_index(0)\n",
    "tgt_vocab.set_default_index(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfbc422-2ca0-43f2-b960-046240ad02df",
   "metadata": {},
   "source": [
    "#### Now we can finally create our collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c903bc3-55bb-4de1-81fc-288eb36df696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List, Tuple\n",
    "\n",
    "def collate_fn(paired_batch: List[Tuple[str]]):\n",
    "    src_batch = []\n",
    "    tgt_batch = []\n",
    "    for src, tgt in paired_batch:\n",
    "        # tokenization and numericalization\n",
    "        src = src_vocab(src_tokenizer.encode(src, out_type=str))\n",
    "        tgt = tgt_vocab(tgt_tokenizer.encode(tgt, out_type=str))\n",
    "        # adding BOS and EOS token\n",
    "        src = torch.cat((torch.tensor([BOS_IDX]),\n",
    "                         torch.tensor(src),\n",
    "                         torch.tensor([EOS_IDX])))\n",
    "        tgt = torch.cat((torch.tensor([BOS_IDX]),\n",
    "                         torch.tensor(tgt),\n",
    "                         torch.tensor([EOS_IDX])))\n",
    "        src_batch.append(src)\n",
    "        tgt_batch.append(tgt)\n",
    "    # padding\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9cfb1-bdd6-47ff-aba5-068875566f53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7455d94e-e164-4b6b-b8d4-bf60b542e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_iter, batch_size=10, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314658e-728c-427a-af11-f9d442371d0a",
   "metadata": {},
   "source": [
    "A little test on the return result of our dataloader.\\\n",
    "Note how the padding is done __vertically__. Each column of the 2-dimensional matrix is a sentence (either source or target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf0eded-3443-4ce4-a663-8a2b92bec064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [   29,   216,    20,    41, 27230,    50,    11,   163,  1106,  3211],\n",
      "        [   18,  1740,    29,    18,   782,   534,  1200,   234,   503,  7850],\n",
      "        [   11,    13,   166,    38,  1309,  9652,    32, 21288,   188, 24129],\n",
      "        [  785,  2535,     5,  2617,  1958,    15,    63,  9523,  2085,   326],\n",
      "        [    3,   571,     4,   819,   821,    50,   172,  4065,   377,     3],\n",
      "        [    1,     3,  3461,    13,  1221,    44,  6958,   234,   436,     1],\n",
      "        [    1,     1,    63, 12957,     4,  1053,  8456,  5249,  7664,     1],\n",
      "        [    1,     1, 16195,  2911,     7,     3,  2930, 13536,   369,     1],\n",
      "        [    1,     1,  1733,    12,    16,     1,    30,    36,     3,     1],\n",
      "        [    1,     1,  3891,  5605,     5,     1,   500,     8,     1,     1],\n",
      "        [    1,     1,  6807,  9401,    33,     1,     6,   163,     1,     1],\n",
      "        [    1,     1,     8,    13,    17,     1,   471,    94,     1,     1],\n",
      "        [    1,     1,  1716, 17741,    17,     1,     6,    13,     1,     1],\n",
      "        [    1,     1,   766,    23, 12066,     1,   582,    89,     1,     1],\n",
      "        [    1,     1,    39,     5,   750,     1,   959,  2869,     1,     1],\n",
      "        [    1,     1,     4,  1186,    91,     1,    28,   226,     1,     1],\n",
      "        [    1,     1,   271,    20,   654,     1,    25,   234,     1,     1],\n",
      "        [    1,     1,    21,    52,   244,     1,   103,   850,     1,     1],\n",
      "        [    1,     1,    23, 11552,     4,     1,   298,    12,     1,     1],\n",
      "        [    1,     1,   161,  3265, 11889,     1,  1023,    24,     1,     1],\n",
      "        [    1,     1,  1981,  1873,     8,     1,    47,     3,     1,     1],\n",
      "        [    1,     1,  1170,     6,    22,     1,     3,     1,     1,     1],\n",
      "        [    1,     1,  7984,     3,    55,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     6,     1,   695,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     3,     1,  1010,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     5,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     7,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,   372,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,    78,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,  2161,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,    26,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1, 13997,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     3,     1,     1,     1,     1,     1]])\n",
      "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [ 1397,   627,  7900,  8108, 13159, 25423,  1452,  2833,  9750, 14694],\n",
      "        [  112,  7726,  3776,  3066,  4455,  9936,  2288,    65,    92, 20803],\n",
      "        [  491,  1053, 13749,  3435,   520,     4,  5880, 13850, 26037,   228],\n",
      "        [    3,     3,  1967,    42,  1571,   312,  1693,  4372,  4914,   942],\n",
      "        [    1,     1, 15232,   584,  1000, 10264,    13,  2386, 18070,  4015],\n",
      "        [    1,     1,  2642,  4918,    81,  1887,   503,  4038,  1642, 23677],\n",
      "        [    1,     1,   110, 15033,     8,     3,     4,    65,   374,    35],\n",
      "        [    1,     1,   339,   535,    21,     1,   245,   358,     3,     3],\n",
      "        [    1,     1,     5,     4,     9,     1,   645,  8822,     1,     1],\n",
      "        [    1,     1,     3,   206,     9,     1,    20,  1356,     1,     1],\n",
      "        [    1,     1,     1, 18304,   399,     1,    17, 17185,     1,     1],\n",
      "        [    1,     1,     1,    12,  8111,     1,    59,  2274,     1,     1],\n",
      "        [    1,     1,     1, 15033,  4157,     1,   686,   996,     1,     1],\n",
      "        [    1,     1,     1,   154, 18688,     1,    16,    65,     1,     1],\n",
      "        [    1,     1,     1,     5,     4,     1,     3,  2251,     1,     1],\n",
      "        [    1,     1,     1,     3, 10031,     1,     1,    15,     1,     1],\n",
      "        [    1,     1,     1,     1,    12,     1,     1,     3,     1,     1],\n",
      "        [    1,     1,     1,     1,  1488,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,  9043,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,  9042,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,    14,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,   152,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,   514,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,   915,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,    13,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,   240,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,  2802,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,  3016,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,    10,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,  3655,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,    16,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1, 19338,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     5,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     3,     1,     1,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "for src_batch, tgt_batch in train_dataloader:\n",
    "    print(src_batch)\n",
    "    print(tgt_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610287a3-57f7-45a7-b4ef-63448d5c4475",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build <i>transformer</i> model for MT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16671dd-da3d-425f-b56b-289592aecc99",
   "metadata": {},
   "source": [
    "I will first give out the whole model code then explain some details that might be necessary or helpful. The major part of PyTorch's model codes are usually quite self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98aa5e1a-e971-4fea-ac83-a8307977221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to demonstration need, especially for explaning masking later, I will need to define 'DEVICE' first in this notebook.\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d4704-bd06-4f95-a7f2-dc814d302aab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6537089-e77e-4bc5-abe3-1778761167d8",
   "metadata": {},
   "source": [
    "I maninly copied the code from PyTorch's <a href='https://pytorch.org/tutorials/beginner/translation_transformer.html'>tutorial</a>, with a few modifications.\n",
    "\n",
    "The most important change I've made is that I deleted the <code>src_mask</code> parameter in <code>class Seq2SeqTransformer</code>'s <code>def forward</code> and <code>def encode</code>. For NMT task, you don't need to mask the source to prevent further token's information from leaking. For anyone who is not very familiar with masking in NMT, please refer to the \"Masking and attention\" section in the following. The <code>src_mask</code> created in PyTorch is just a tensor full of boolean 'False', which has no influence on the computation whatsoever.\n",
    "\n",
    "Note that <code>def encode</code> and <code>def decode</code> are only used during inference. We do not need them during training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "221778a4-b18f-440a-8f55-7d418747659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71984c3a-34e6-4f90-9406-c1000182c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 maxlen: int = 5000):\n",
    "        super().__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0)])\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 2048,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, \n",
    "                                src_mask=None, tgt_mask=tgt_mask, memory_mask=None,\n",
    "                                src_key_padding_mask=src_padding_mask,\n",
    "                                tgt_key_padding_mask=tgt_padding_mask, \n",
    "                                memory_key_padding_mask=src_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)))\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f5394-cf89-428c-8ba5-e794e410387f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## nn.Module and 'def forward'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd6b0e-ef11-4600-8ac3-f1d7908b34c7",
   "metadata": {},
   "source": [
    "A great benifit of modeling with PyTorch is that you can subclass nn.Module and its subclasses to fastly create a custom neural layer/network.\\\n",
    "The logic is quite simple: you first create the layer/network using <code>\\_\\_init\\_\\_</code>, in which you define necessary layers and save them in a <code>self</code> variable. Then you create a <code>forward</code> function (the name \"forward\" is predefined, do not change it) to define how the input data to your layer/network should be processed.\\\n",
    "Take above <code>TokenEmbedding</code> class as example, we only need a <code>nn.Embedding</code> instance, then in <code>forward</code>, we want our input data (in the scope of NMT and PyTorch, it usually is a <code>Tensor[int]</code> filled with token ids to first pass a normal embedding layer then to be multiplied by the square root of the embedding size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f62d7a-1b7c-4ebc-ae0f-e40849560e91",
   "metadata": {},
   "source": [
    "The usage of nn.Module's subclasses is also quite intuitive: first you create the instance with necessary parameters, then you just pass in the data directly into the instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07b6f565-27d9-440f-beeb-39ba94dc0da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "tensor([[ -0.1425,  34.4576, -24.0051,  ...,  13.5429,  30.8963,  -9.9119],\n",
      "        [-16.2745,  25.4573,   3.7499,  ...,   3.6479, -35.2485,  20.5989],\n",
      "        [-14.3610,  -9.9353,  26.1410,  ...,  21.3823, -18.6943,  -0.0691],\n",
      "        ...,\n",
      "        [ 12.2563,  -7.4711,  -1.8407,  ..., -28.6219,  17.4591, -46.8772],\n",
      "        [ 12.2563,  -7.4711,  -1.8407,  ..., -28.6219,  17.4591, -46.8772],\n",
      "        [ 12.2563,  -7.4711,  -1.8407,  ..., -28.6219,  17.4591, -46.8772]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(69)\n",
    "test_token = torch.tensor([2, 5, 6, 7, 3, 1, 1, 1])\n",
    "\n",
    "token_embedding = TokenEmbedding(len(src_vocab), 512)\n",
    "r = token_embedding(test_token)\n",
    "print(r.size())  # each token_id becomes a embedding of size 512\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0616d-e3dd-4c46-a890-b9b155f8d633",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Token embedding and positional encoding: batch, unsqueeze, and broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16479b80-1991-4680-a275-7d0b9dac25d7",
   "metadata": {},
   "source": [
    "When I was reading PyTorch's <a href='https://pytorch.org/tutorials/beginner/translation_transformer.html'>tutorial</a> on translation task, I was confused by this line in <code>PositionalEncoding</code>'s init method.\n",
    "```python\n",
    "pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "```\n",
    "Put simply, the <code>.unsqueeze()</code> is adding a singleton dimension (dimension that has a length of 1) to the positional encoding matrix so that the following line of code can be executed without error.\n",
    "```python\n",
    "token_embedding + self.pos_embedding[:token_embedding.size(0)]\n",
    "```\n",
    "Following I created a pseudo test example to show how <code>token_embedding</code> cannot be added to <code>pos_embedding</code> if <code>unsqueeze</code> is not used.\\\n",
    "I shall emphasize that the token embeddings is 3-dimensional tensor because of batching in our <code>DataLoader</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0721540-49be-4576-b818-4f01b2a0fbad",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m test_token_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# padding length = 5, batch_size = 3, emb_size = 10\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_positional \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# padding length = 5, emb_size = 10\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtest_token_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_positional\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(69)\n",
    "\n",
    "test_token_embeds = torch.randn(5, 3, 10)  # padding length = 5, batch_size = 3, emb_size = 10\n",
    "test_positional = torch.ones(5, 10)  # padding length = 5, emb_size = 10\n",
    "test_token_embeds + test_positional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94101d3-4be7-477b-a6b1-71c42b195d00",
   "metadata": {},
   "source": [
    "The error is thrown basically because the two tensors cannot be added up based on their shape.\\\n",
    "When using <code>.unsqueeze(index)</code>, a singleton dimension is added to the corresponding index of tensor.size():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8feaefa-df7b-4a39-97e2-5d1f42945fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "torch.Size([5, 1, 10])\n",
      "torch.Size([5, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones(5, 10).size())  # shape is [5, 10]\n",
    "print(torch.ones(5, 10).unsqueeze(-2).size())  # 1 is added to the -2 index of [5, 10]\n",
    "print(torch.ones(5, 10).unsqueeze(1).size())  # same as using -2 as index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f8b7d-2d83-46c3-97d8-fa39adfbbfc3",
   "metadata": {},
   "source": [
    "A singleton dimension can resolve the tensor operation issue is because array broadcasting is generally used for tensors. Positional embeddings will be duplicated across token embeddings' batch size dimension.\\\n",
    "For details, please read through this <a href='https://numpy.org/doc/stable/user/basics.broadcasting.html#general-broadcasting-rules'>documentation</a> of NumPy.\n",
    "<p style=\"text-align:center;\"><img src='./images/array_broadcasting.png' style='max-width: 50%; height: auto;'></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f039b-a64f-46cc-bc6d-adf7d9e154f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Masking and attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c4e6a-b5ad-4771-a1a7-6966ef25b36b",
   "metadata": {},
   "source": [
    "Before starting to train the model, there's one last thing we need to consider: masking. _Transformer_ is highly parallelizable because structurally you don't need to train from start position to end position of an bitext example in the case of machine translation, instead, you can feed a whole training example all at once each time.\n",
    "\n",
    "For example, RNN models probably need to train 5 times on example ('This is a test.', 'Este es un test.'):\\\n",
    "('This is a test.', 'Este')\\\n",
    "('This is a test.', 'Este es')\\\n",
    "('This is a test.', 'Este es un')\\\n",
    "('This is a test.', 'Este es un test')\\\n",
    "('This is a test.', 'Este es un test.')\n",
    "\n",
    "_Transformer_ leverages \"masking\" to prevent future information from leaking into each position in the self-attention layer of the decoder by setting future position attention weights to zero:\n",
    "\n",
    "<p style=\"text-align:center;\"><img src='./images/self_attentin_weights_matrix.png' style='max-width: 20%; height: auto;'></p>\n",
    "\n",
    "This way, when this self-attentin weights matrix of size (length, length) is multiplied with the training example input matrix of size (length, embedding size), the result vector of postion 0 'Este' will have no information from later positions. On the other hand, since source is always known beforehand whether during training, evaluation or inference, you don't need to mask source attention weight matrix at all. That's why we delete the <code>src_mask</code> parameter when constructing the model.\n",
    "\n",
    "Since PAD_IDX is useless information, padding masking is usually combined with attention masking as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622289f-29cf-44fd-9438-119e8b9f1334",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d23d60e-f6c2-4143-9e48-679982b42e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "    \n",
    "    tgt_mask = Transformer.generate_square_subsequent_mask(tgt_seq_len).to(DEVICE)\n",
    "    \n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    \n",
    "    return tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15bf1212-b26c-46a8-8aad-0b447acd9dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_batch, tgt_batch)\n",
    "print(tgt_mask)\n",
    "print(tgt_padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd4aca-fe9c-4b79-bf9b-c42a7b346577",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### PyTorch's implementation (skip if not interested)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e12e8-c06f-4064-87de-8c8c3278c420",
   "metadata": {},
   "source": [
    "In this section I will try to explain how exactly <code>torch.nn.Transformer</code> leverages the attention_mask and padding_mask to generate attention_weights matrix where future position or padding position is set to 0.\n",
    "\n",
    "attention_mask and padding_mask are essentially required by the <code>from .activation import MultiheadAttention</code> used in <code>TransformerEncoderLayer</code> and <code>TransformerDecoderLayer</code> (find <a href='https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer'>here</a>.)\n",
    "\n",
    "The source code <code>MultiheadAttention</code> is here: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py#L888"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8bfbbd-a405-48da-a95e-b7892754c17a",
   "metadata": {},
   "source": [
    "##### TL;DR\n",
    "\n",
    "1. Fill attention mask and padding mask with 0 for position not required of masking and with '-inf' for where required. If any of the two masks are of dtype torch.bool, fill 0 where False and '-inf' where True.\n",
    "2. Expand and reshape padding mask to size (batch_size * num_head, 1, pad_length)\n",
    "3. Merge attention and padding mask to get final attention of size ((batch_size * num_head, pad_length, pad_length))\n",
    "4. Use torch.baddbmm(final_attention_mask, Q, K) then softmax to generate an attention weight matrix where positions that need masking are set to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebabadc8-2641-4947-9bc7-d20c607e252e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d291dc2-5f44-4e5a-a8dd-fbd1df0401e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change padding mask's dtype\n",
    "\n",
    "tgt_padding_mask = torch.zeros_like(tgt_padding_mask, dtype=tgt_mask.dtype).masked_fill_(tgt_padding_mask, float(\"-inf\"))\n",
    "tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1abed04-682e-45a1-b546-e0601c79ace1",
   "metadata": {},
   "source": [
    "First of all, we need to merge attention mask and padding mask. Note that padding mask is different for each exmaple in a batch, while attention mask is the same for each exmaple as long as it's the same batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc2440b4-25c9-4cc1-b7e8-4639155bbb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 35])\n",
      "torch.Size([10, 35])\n"
     ]
    }
   ],
   "source": [
    "print(tgt_mask.size())  # (pad_length, pad_length)\n",
    "print(tgt_padding_mask.size())  # (batch_size, pad_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3cc5b6-5338-4185-9253-0ab4c1116a1f",
   "metadata": {},
   "source": [
    "Create a (80, 1, 35) tensor from <code>tgt_padding_mask</code>, representing 10 batches * 8 heads of each example, each example have its own padding mask. <code>temp[:8]</code> are 1st sentence of the batch, <code>temp[8:16]</code> are the 2nd sentence of the batch, and so on.\n",
    "Since attention mask is the same for all sentences in the batch, we can simply leverage broadcasting to expand the second dimension of <code>temp</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a695a433-34b8-40e3-9140-eae426014335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 35, 35])\n"
     ]
    }
   ],
   "source": [
    "temp = tgt_padding_mask.view(10, 1, 1, 35).expand(-1, 8, -1, -1).reshape(10 * 8, 1, 35)\n",
    "merged_mask = tgt_mask + temp\n",
    "print(merged_mask.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a9df0-5a92-47cd-a24f-a98d6a1b9d0d",
   "metadata": {},
   "source": [
    "Note how the 1st sentence's merged mask does not change starting from position 4, because if you go back and check the <code>tgt_batch</code>, you will notice that position 5-34 are all PAD_IDX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6666388d-acba-487a-8713-ad4104f14566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]])\n",
      "****************************************************\n",
      "tensor([[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]])\n"
     ]
    }
   ],
   "source": [
    "print(merged_mask[0:1, :10])  # 1st sentence, 1th head, position 0 - 9\n",
    "print('****************************************************')\n",
    "print(merged_mask[9:10, :10])  # 2nd sentence, 1th head, position 0 - 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec570fa-7d2c-4590-b0b6-0a361297c118",
   "metadata": {},
   "source": [
    "Let's create a pseudo input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92c16393-be24-4006-aee6-f592158dd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(69)\n",
    "token_emb = TokenEmbedding(vocab_size=len(tgt_vocab), emb_size=512)\n",
    "pos_enco = PositionalEncoding(emb_size=512, dropout=0, maxlen=200)\n",
    "x = pos_enco(token_emb(tgt_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5d46772-01e0-47aa-a54f-209db6845ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 10, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()  # (pad_length, batch_size, emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c14e62c-a6f2-4d1a-8dd6-0603091d63db",
   "metadata": {},
   "source": [
    "Create $W^Q$, $W^K$ and $W^V$ and initialize their weights in order to later get the $Q$, $K$ and $V$.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src='./images/multihead_attention.png' style='max-width: 60%; height: auto;'></p>\n",
    "\n",
    "Note that since each head's dimension size is calculated from emb_size/num_heads in PyTorch, so PyTorch actually outputs all heads' information altogether, that why <code>in_proj_weight</code> 's 2nd dimension size is $512 = num\\_heads * head\\_dim = num\\_heads * emb\\_size/num\\_heads = emb\\_size$\n",
    "\n",
    "Besides, PyTorch also stacks $W^Q$, $W^K$ and $W^V$ and learn them together, which is why the 1st dimension of <code>in_proj_weight</code> is $3 * 512$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29c75f5f-60a3-439e-9175-9f54d4cb41a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536, 512])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0368,  0.0329, -0.0410,  ..., -0.0434, -0.0197,  0.0376],\n",
      "        [ 0.0194,  0.0408,  0.0345,  ...,  0.0212, -0.0377,  0.0320],\n",
      "        [-0.0274,  0.0509,  0.0161,  ..., -0.0350, -0.0225,  0.0438],\n",
      "        ...,\n",
      "        [ 0.0262,  0.0330,  0.0480,  ...,  0.0261, -0.0299, -0.0493],\n",
      "        [ 0.0397, -0.0243, -0.0246,  ...,  0.0453,  0.0184,  0.0262],\n",
      "        [-0.0028,  0.0334,  0.0405,  ...,  0.0307,  0.0399,  0.0266]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "in_proj_weight = Parameter(torch.empty((3 * 512, 512)))  # Stack of W^q, W^k, W^v with all 8 heads\n",
    "xavier_uniform_(in_proj_weight)  # initialize weight matrix for demo usage\n",
    "print(in_proj_weight.size())\n",
    "print(in_proj_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e29f94c-1da0-46de-bf9c-c9263f0ee9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 10, 512])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = torch._C._nn.linear\n",
    "y = linear(x, in_proj_weight, bias=None)  # W^o\n",
    "q, k, v = y.chunk(3, dim=-1)\n",
    "q.size()  # here the q, k, v are of num_heads * head_dim dimension, putting all 8 heads together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c532e80-6db2-427e-9f85-6c31e6cd786e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 35, 64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape q, k, v to be: 10 batches of 8 heads, then (pad_length, head_dim)\n",
    "q = q.contiguous().view(35, 10 * 8, int(512/8)).transpose(0, 1)\n",
    "k = k.contiguous().view(35, 10 * 8, int(512/8)).transpose(0, 1)\n",
    "v = v.contiguous().view(35, 10 * 8, int(512/8)).transpose(0, 1)\n",
    "q.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03bdcb8-e0f0-4ce1-b8b3-a39eeefc96bc",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"text-align:center;\"><img src='./images/attention_calculation.png' style='max-width: 30%; height: auto;'></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46dae8cd-135f-449c-b216-643fdad5c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_scaled = q / math.sqrt(64)  # first scaled by the squared root of head dimension\n",
    "attn_output_weights = torch.baddbmm(merged_mask, q_scaled, k.transpose(-2, -1))  # then multiplied by K^T, taking into account the masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffb414-c5a0-4843-9594-3d6fb31a0516",
   "metadata": {},
   "source": [
    "<code>torch.baddbmm()</code> is essentially multiply 2nd and 3rd tensor provided, and then add the 1st tensor to the result.\n",
    "\n",
    "See documentation <a href='https://pytorch.org/docs/stable/generated/torch.baddbmm.html'>here.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e07a3bcd-3408-4d3b-b79f-4fa487236ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 35, 35])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed84a6-65ea-4484-aad9-3450a2bc9bd4",
   "metadata": {},
   "source": [
    "Note how all -inf in <code>merged_mask[9:10, :10]</code> has the same position in <code>attn_output_weights[9:10, :10]</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8416330c-2918-42b5-96fc-594727e29d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  44.9964,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf],\n",
       "         [ 230.9838,  -93.6256,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf],\n",
       "         [ -67.1591,  277.1063, -130.4532,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf],\n",
       "         [ 140.4525,   26.3431,  124.3974, -157.7820,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf],\n",
       "         [-452.5932, -265.8404, -160.3563,  110.6875,  -75.1813,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf],\n",
       "         [-116.2640, -166.3753,  301.0557,  300.1140,  277.4109,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf],\n",
       "         [-112.7967, -165.6224,  304.9301,  297.2736,  274.3484,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf],\n",
       "         [-110.2087, -165.8629,  306.7473,  297.1173,  271.5576,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf],\n",
       "         [-109.7131, -168.4476,  305.4208,  299.7612,  270.1816,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf],\n",
       "         [-111.4888, -172.8903,  301.9288,  303.9858,  270.8362,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,\n",
       "               -inf,      -inf,      -inf,      -inf,      -inf]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights[9:10, :10]  # 2nd sentence 1st head, position 0-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9c562-e3d1-4271-b186-b045e3a57b8b",
   "metadata": {},
   "source": [
    "'-inf' will not influence softmax function and will be set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69ae1b9c-c361-4eb6-9e3e-a1484d572c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 1.1335e-01, 8.8665e-01, 3.5569e-15])\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 1.1335e-01, 8.8665e-01, 3.5569e-15,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "# using position 9's raw weigh result without '-inf' to manually calculate softmax\n",
    "# note how the result is the same as passing tensor with '-inf' as the next code cell\n",
    "print(softmax(torch.Tensor([-111.4888, -172.8903,  301.9288,  303.9858,  270.8362]), dim=0))\n",
    "print(softmax(attn_output_weights, dim=-1)[9:10, 9:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c9d48e-52c2-4ad4-9e8e-88b28b222cce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d805c-07a9-480b-8734-3b1e19823791",
   "metadata": {},
   "source": [
    "Now that we have everything we need, let's start the actually simple training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5fae309-1070-49cd-a4b0-1e200d12030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define training parameters \"\"\"\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(src_vocab)\n",
    "TGT_VOCAB_SIZE = len(tgt_vocab)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "DIM_FEEDFORWARD = 2048\n",
    "DROPOUT = 0.1\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, DIM_FEEDFORWARD,\n",
    "                                 DROPOUT)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1213b438-5242-4952-812c-f2314e4308b6",
   "metadata": {},
   "source": [
    "Note that in PyTorch's tutorial, loss of each epoch is defined as losses / len(val_dataloader).\n",
    "\n",
    "According to PyTorch's <a href='https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader'>documentation</a>, \"len(dataloader) heuristic is based on the length of the sampler used. When dataset is an IterableDataset, it instead returns an estimate based on len(dataset) / batch_size\", which is steps (number of batches).\n",
    "\n",
    "Since I did not define <code>\\_\\_len\\_\\_</code> for our <code>MyIterableDataset</code> class, I simply use <code>num_batches</code> to register in the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb28ac7a-8e53-4491-b673-4fbcf57397f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define train and evaluation epoch function \"\"\"\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = MyIterableDataset('./data/train_test')\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    num_batches = 0\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        num_batches += 1\n",
    "        \n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        tgt_input = tgt[:-1, :]  # we only need till the penultimate position to predict the last position\n",
    "\n",
    "        tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, tgt_mask, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / num_batches\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    val_iter = MyIterableDataset('./data/val_test')\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    num_batches = 0\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        num_batches += 1\n",
    "        \n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, tgt_mask,src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df0528-72e2-43a6-bb15-d765c2d9dda9",
   "metadata": {},
   "source": [
    "I added an simple early stopping rule to the training loop based on PyTorch's tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917ca6f-f121-4d0f-8b95-1da9eb64799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Training loop \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "MAX_EPOCHS = 10\n",
    "PATIENCE = 5\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience: int=5):\n",
    "        self.patience = patience\n",
    "        self.last_val_loss = np.inf\n",
    "        self.loss_increase_count = 0\n",
    "    \n",
    "    def stop_decider(self, val_loss):\n",
    "        if val_loss < self.last_val_loss:\n",
    "            self.last_val_loss = val_loss\n",
    "            self.loss_increase_count = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.last_val_loss = val_loss\n",
    "            self.loss_increase_count += 1\n",
    "            if self.loss_increase_count >= self.patience:\n",
    "                return True\n",
    "\n",
    "earlystopper = EarlyStopper(PATIENCE)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    if earlystopper.stop_decider(val_loss):\n",
    "        print(f\"Training has finished early because the evaluation loss hasn't improved for {PATIENCE} epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7baa7c-d91f-421c-8b2f-8b35964c5d39",
   "metadata": {},
   "source": [
    "I trained 10 epochs using Google Colab's free GPU on a training set of 10,000 pairs of bitext and an evaluation set of 2,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd239d39-f2ab-4b0d-a1dc-f20625805c4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Plot out loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8942f5-3b30-43ab-a7f3-402a2ce83fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying results from training in Google Colab\n",
    "\n",
    "train_losses = [8.053573,7.229754,6.931107,6.755049,6.618917,6.509467,6.413544,6.315699,6.224345,6.140398]\n",
    "val_losses = [7.659284,7.492211,7.485783,7.436596,7.509234,7.468005,7.468515,7.465234,7.547200,7.577723]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9359d238-44a0-4c02-bda9-73abf26b6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71b30580-e1cc-4f2f-a06c-b6f5cbc4960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'epoch':[_ for _ in range(1,len(train_losses)+1)],\n",
    "                   'train_losses':train_losses, \n",
    "                   'val_losses':val_losses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd602bf-4f72-4f96-83a6-ef71f2ac81f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_losses</th>\n",
       "      <th>val_losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.053573</td>\n",
       "      <td>7.659284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.229754</td>\n",
       "      <td>7.492211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6.931107</td>\n",
       "      <td>7.485783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6.755049</td>\n",
       "      <td>7.436596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6.618917</td>\n",
       "      <td>7.509234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6.509467</td>\n",
       "      <td>7.468005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6.413544</td>\n",
       "      <td>7.468515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>6.315699</td>\n",
       "      <td>7.465234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>6.224345</td>\n",
       "      <td>7.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>6.140398</td>\n",
       "      <td>7.577723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_losses  val_losses\n",
       "0      1      8.053573    7.659284\n",
       "1      2      7.229754    7.492211\n",
       "2      3      6.931107    7.485783\n",
       "3      4      6.755049    7.436596\n",
       "4      5      6.618917    7.509234\n",
       "5      6      6.509467    7.468005\n",
       "6      7      6.413544    7.468515\n",
       "7      8      6.315699    7.465234\n",
       "8      9      6.224345    7.547200\n",
       "9     10      6.140398    7.577723"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b364cd01-2cae-438b-ac1a-2affe79bf931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(df['epoch'], df['train_losses'], label='training loss')\n",
    "plt.plot(df['epoch'], df['val_losses'], label='evaluation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.legend(loc=[1.05, 0.7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9df7f-3f35-42bd-9bc5-8041d96d0767",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d42545-2b6f-46b0-8a9d-93daa8663c3d",
   "metadata": {},
   "source": [
    "Now that we have the model trained, we can use it to decode and generate translation.\n",
    "\n",
    "I did not change the logics behind PyTorch's tutorial codes, only adapting them to our scenarios, especially in the <code>translate</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5405f6d-0a73-46c0-b1e4-7de051946667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I do not have a GPU machine, so I need to use map_location parameter.\n",
    "\n",
    "model = torch.load('model_colab', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a1abbe2-8e38-4580-8696-8bdebeb8b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    memory = model.encode(src)\n",
    "    \n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (Transformer.generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)  # last output vector\n",
    "        out = out.transpose(0, 1)  # adjust batching\n",
    "        prob = model.generator(out[:, -1])  # softmax layer, return probability distribution\n",
    "        _, next_word = torch.max(prob, dim=1)  # get the Tensor containing the token_id with highest probability\n",
    "        next_word = next_word.item()  # convert Tensor to int\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src: str):\n",
    "    model.eval()\n",
    "    \n",
    "    src = src_vocab(src_tokenizer.encode(src, out_type=str))\n",
    "    src = torch.cat((torch.tensor([BOS_IDX]),\n",
    "                     torch.tensor(src),\n",
    "                     torch.tensor([EOS_IDX]))).view(-1, 1)\n",
    "    print(src)\n",
    "    \n",
    "    num_tokens = src.shape[0]\n",
    "    \n",
    "    tgt = greedy_decode(model, src, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    print(tgt)\n",
    "    tgt = tgt_vocab.lookup_tokens(list(tgt.cpu().numpy()))\n",
    "    tgt = tgt_tokenizer.decode(tgt).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").strip()\n",
    "    \n",
    "    return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8e784-e7f4-4cd8-9041-d9bc9e0a3bed",
   "metadata": {},
   "source": [
    "The output is still gibberish considering the training data volume and only 10 epochs of train.\n",
    "\n",
    "But hey! It's been a long journey and it really works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c327d89-deca-44d1-a903-b7de72537eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2],\n",
      "        [  29],\n",
      "        [  18],\n",
      "        [  11],\n",
      "        [1120],\n",
      "        [ 785],\n",
      "        [   6],\n",
      "        [   3]])\n",
      "tensor([ 2,  6,  8, 32, 75,  4,  3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1K我们,'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, 'this is a simple test.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
